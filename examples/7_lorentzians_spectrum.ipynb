{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28cb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baynes.model_utils import get_model, inits_from_priors\n",
    "from baynes.plotter import FitPlotter\n",
    "from scipy import stats\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cmdstanpy\n",
    "import logging\n",
    "cmdstanpy.utils.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a784d34",
   "metadata": {},
   "source": [
    "# Fit of any number of lorentzian peaks convolved with a gaussian"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbfd04dd",
   "metadata": {},
   "source": [
    "### Generate the data \n",
    "Any number of lorentzian peak can be generated, each with a different mean, width and height. A common gaussian spread simulating the experimental resolution is added, then the data is binned and plotted.\n",
    "\n",
    "$$P(x) = \\sum_{i=1}^{n\\_peaks} heights_i\\cdot Lor(x, E0_i, gamma_i)\\otimes Normal(x, 0, FWHM)$$\n",
    "\n",
    "Other than testing the convolution functions, this example demonstrates various non-trivial constraints on the parameters. In this case, the centers of the lorentzians are ordered in order to avoid splitting their posterior distributions, while the sum of the relative heights must be equal to 1. See in the stan model how these constraints are enforced in the parameter's definitions and with specific priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_peaks = 3\n",
    "n_events = 5e4\n",
    "n_bins = 200\n",
    "\n",
    "def rand_simplex(size):\n",
    "    h = np.round(np.random.uniform(size=size-1), 3)\n",
    "    h = np.append(h, [0, 1])\n",
    "    h = np.sort(h)\n",
    "    return np.diff(h)\n",
    "\n",
    "E0 = np.sort(np.random.uniform(20,180, size=n_peaks))  # means of the lorentzians\n",
    "gamma = np.random.uniform(1, 15, size=n_peaks)         # widths of the lorenztians\n",
    "heights = rand_simplex(n_peaks)                        # relative heights of the peaks, their sum must be 1\n",
    "spread = np.random.uniform(1,5)                       # experimental resolution's spread\n",
    "\n",
    "events = np.array([])\n",
    "for i in range(n_peaks):\n",
    "    events = np.append(events, stats.cauchy.rvs(loc=E0[i],\n",
    "                                                scale=gamma[i]/2,\n",
    "                                                size=int(heights[i]*n_events)))  # generate the events\n",
    "n_events = len(events)                                                           # correct discrepancies from rounding\n",
    "events = events + np.random.normal(scale = spread, size=n_events)                # add gaussian spread\n",
    "\n",
    "true_pars = list(it.chain.from_iterable([E0, gamma, heights, [spread * 2 * np.sqrt(2*np.log(2))]]))\n",
    "counts, edges = np.histogram(events, bins=np.arange(0,n_bins,1))\n",
    "centers = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "plt.plot(centers, counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8f56c41",
   "metadata": {},
   "source": [
    "### Assemble the data.\n",
    "The model fits the binned data and requires the number of bins of the gaussian window that will be convolved with the true response to be specified. Due to the implementation of the discrete convolution, this value $N\\_window$ must be odd, otherwise the results will be shifted by 1 bin. The variables starting with $p\\_$ are the prior's parameters and can be set with a rough guess from the previous graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79de7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_window = 51\n",
    "p_x0 = [20, 50, 160]\n",
    "p_g = [10, 10, 10]\n",
    "p_h = [0.5, 0.3, 0.2]\n",
    "p_FWHM = 10\n",
    "\n",
    "data={\"N_bins\": len(counts),\n",
    "      'N_peaks': n_peaks,\n",
    "      'counts': counts.tolist(),\n",
    "      'x': edges.tolist(),\n",
    "      'p_FWHM': p_FWHM,\n",
    "      'p_gamma': p_g,\n",
    "      'p_i': p_h,\n",
    "      'p_E0': p_x0,\n",
    "      'prior': 1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69b98f02",
   "metadata": {},
   "source": [
    "### Sample from the priors\n",
    "Plot a prior predictive check and the sampled prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(\"lorentzians.stan\")\n",
    "prior_fit = model.sample(data,\n",
    "                         chains=4,\n",
    "                         iter_warmup=100,\n",
    "                         iter_sampling=1000)\n",
    "\n",
    "plotter = FitPlotter(prior_fit, fit_title='prior_fit')\n",
    "plotter.predictive_check('counts_rep',\n",
    "                         data=data,\n",
    "                         data_key='counts')\n",
    "plotter.kde_plot(hue='variable')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dbecbc0",
   "metadata": {},
   "source": [
    "### Prepare the initializations of each chain\n",
    "Since this fit is quite complex, sampling with the default initial points will converge really slowly or not at all. Suitable starting values for each chain can be drawn from the prevously sampled priors by using $inits\\_from\\_priors$. This function creates a .json file containing the inits for each chain and returns the filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chains = 4\n",
    "init_files = inits_from_priors(model, prior_fit, n_chains)\n",
    "print(init_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1223e6e8",
   "metadata": {},
   "source": [
    "### Sample from the model\n",
    "This usually requires some minutes. Due to the convolution, the performance mainly depends from the number of bins used for the gaussian window and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['prior'] = 0\n",
    "fit = model.sample(data,\n",
    "                   chains=4,\n",
    "                   iter_warmup=500,\n",
    "                   iter_sampling=500,\n",
    "                   save_warmup=True,\n",
    "                   inits=init_files)\n",
    "plotter.add_fit(fit, fit_title='lorentzian_fit')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a56546d0",
   "metadata": {},
   "source": [
    "Confirm the convergence by checking the HMC diagnostics and print a summary for all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit.diagnose())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30cc011c",
   "metadata": {},
   "source": [
    "Plot the convergence of each parameter, the posterior predictive check and the posterior distributions. As can be seen, all the posteriors converge to the true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7038f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.add_fit(fit, fit_title='lorentzian_fit')\n",
    "plotter.convergence_plot(initial_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ca1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.predictive_check('counts_rep',\n",
    "                         data=data,\n",
    "                         data_key='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ad4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pars = ['E0', 'gamma', 'i', 'FWHM']\n",
    "plotter.kde_plot(parameters=plot_pars, hue='variable')\n",
    "plt.close()\n",
    "plotter.add_lines(true_pars)\n",
    "plotter.get_current_figure()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7967d4c3",
   "metadata": {},
   "source": [
    "For models with many variables, $pair\\_grid$ allows to inspect the correlations between them. In this case correlations are stronger between $\\Gamma_i$ and $FWHM$, since they both contribute to the total width of the peaks, and between the relative heights, since they are a unit vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae3044",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.pair_grid(parameters=plot_pars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "212996f0",
   "metadata": {},
   "source": [
    "This workflow can be automatically executed with standard_analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baynes import standard_analysis\n",
    "fit = standard_analysis(model, data, plotter,\n",
    "                        sampler_kwargs={'chains':4, 'iter_warmup': 500},\n",
    "                        plot_params=plot_pars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baynesenv",
   "language": "python",
   "name": "baynesenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
